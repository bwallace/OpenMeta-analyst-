<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>R: Generalized additive models with integrated smoothness estimation</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<link rel="stylesheet" type="text/css" href="../../R.css">
</head><body>

<table width="100%" summary="page for gam {mgcv}"><tr><td>gam {mgcv}</td><td align="right">R Documentation</td></tr></table>
<h2>Generalized additive models with integrated smoothness estimation</h2>


<h3>Description</h3>

<p>
Fits a generalized additive model (GAM) to
data, the term `GAM' being taken to include any quadratically penalized GLM.  
The degree of smoothness of model terms is estimated as part of
fitting. <code>gam</code> can also fit any GLM subject to multiple quadratic penalties (including 
estimation of degree of penalization). Isotropic or scale invariant smooths of any number of variables are
available as model terms, as are linear functionals of such smooths; confidence/credible intervals are readily
available for any quantity predicted using a fitted model; <code>gam</code> is extendable: users can add smooths. 
</p>
<p>
Smooth terms are represented using penalized regression splines (or similar smoothers)
with smoothing parameters selected by GCV/UBRE/AIC/REML or by regression splines with
fixed degrees of freedom (mixtures of the two are permitted). Multi-dimensional smooths are 
available using penalized thin plate regression splines (isotropic) or tensor product splines 
(when an isotropic smooth is inappropriate). For an overview of the smooths available see <code><a href="smooth.terms.html">smooth.terms</a></code>. 
For more on specifying models see <code><a href="gam.models.html">gam.models</a></code> and <code><a href="linear.functional.terms.html">linear.functional.terms</a></code>. 
For more on model selection see <code><a href="gam.selection.html">gam.selection</a></code>. 
</p>
<p>
<code>gam()</code> is not a clone of what S-PLUS provides: the major
differences are (i) that by default estimation of the
degree of smoothness of model terms is part of model fitting, (ii) a
Bayesian approach to variance estimation is employed that makes for easier
confidence interval calculation (with good coverage probabilites), (iii) that the model
can depend on any (bounded) linear functional of smooth terms,
and, (iv) the parametric part of the model can be penalized, and 
(v) the facilities for incorporating smooths of more than one variable are
different: specifically there are no <code>lo</code> smooths, but instead (a) <code>s</code>
terms can have more than one argument, implying an isotropic smooth and (b) <code>te</code> smooths are
provided as an effective means for modelling smooth interactions of any
number of variables via scale invariant tensor product smooths. If you want
a clone of what S-PLUS provides use <a href="../../gam/html/gam.html">gam</a> from package <code>gam</code>.
</p>


<h3>Usage</h3>

<pre>

gam(formula,family=gaussian(),data=list(),weights=NULL,subset=NULL,
    na.action,offset=NULL,method="GCV.Cp",
    optimizer=c("outer","newton"),control=gam.control(),scale=0,
    select=FALSE,knots=NULL,sp=NULL,min.sp=NULL,H=NULL,gamma=1,
    fit=TRUE,paraPen=NULL,G=NULL,in.out,...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>formula</code></td>
<td>
A GAM formula (see <code><a href="formula.gam.html">formula.gam</a></code> and also <code><a href="gam.models.html">gam.models</a></code>). 
This is exactly like the formula for a GLM except that smooth terms, <code>s</code> and <code>te</code> can be added 
to the right hand side to specify that the linear predictor depends on smooth functions of predictors 
(or linear functionals of these).
</td></tr>
<tr valign="top"><td><code>family</code></td>
<td>
This is a family object specifying the distribution and link to use in
fitting etc. See <code><a href="../../stats/html/glm.html">glm</a></code> and <code><a href="../../stats/html/family.html">family</a></code> for more
details. A negative binomial family is provided: see <code><a href="negbin.html">negbin</a></code>.
</td></tr>
<tr valign="top"><td><code>data</code></td>
<td>
A data frame or list containing the model response variable and 
covariates required by the formula. By default the variables are taken 
from <code>environment(formula)</code>: typically the environment from 
which <code>gam</code> is called.</td></tr>
<tr valign="top"><td><code>weights</code></td>
<td>
prior weights on the data.</td></tr>
<tr valign="top"><td><code>subset</code></td>
<td>
an optional vector specifying a subset of observations to be
used in the fitting process.</td></tr>
<tr valign="top"><td><code>na.action</code></td>
<td>
a function which indicates what should happen when the data
contain `NA's.  The default is set by the `na.action' setting
of `options', and is `na.fail' if that is unset.  The
``factory-fresh'' default is `na.omit'.</td></tr>
<tr valign="top"><td><code>offset</code></td>
<td>
Can be used to supply a model offset for use in fitting. Note
that this offset will always be completely ignored when predicting, unlike an offset 
included in <code>formula</code>: this conforms to the behaviour of
<code>lm</code> and <code>glm</code>.</td></tr>
<tr valign="top"><td><code>control</code></td>
<td>
A list of fit control parameters returned by 
<code><a href="gam.control.html">gam.control</a></code>.</td></tr>
<tr valign="top"><td><code>method</code></td>
<td>
The smoothing parameter estimation method. <code>"GCV.Cp"</code> to use GCV for unknown scale parameter and
Mallows' Cp/UBRE/AIC for known scale. <code>"GACV.Cp"</code> is equivalent, but using GACV in place of GCV. <code>"REML"</code> 
for REML estimation, including of unknown scale, <code>"P-REML"</code> for REML estimation, but using a Pearson estimate 
of the scale. <code>"ML"</code> and <code>"P-ML"</code> are similar, but using maximum likelihood in place of REML. </td></tr>
<tr valign="top"><td><code>optimizer</code></td>
<td>
An array specifying the numerical optimization method to use to optimize the smoothing 
parameter estimation criterion (given by <code>method</code>). <code>"perf"</code> for performance iteration. <code>"outer"</code> 
for the more stable direct approach. <code>"outer"</code> can use several alternative optimizers, specified in the 
second element of <code>optimizer</code>: <code>"newton"</code> (default), <code>"bfgs"</code>, <code>"optim"</code>, <code>"nlm"</code> 
and <code>"nlm.fd"</code> (the latter is based entirely on finite differenced derivatives and is very slow). </td></tr>
<tr valign="top"><td><code>scale</code></td>
<td>
If this is positive then it is taken as the known scale parameter. Negative signals that the 
scale paraemter is unknown. 0 signals that the scale parameter is 1  for Poisson and binomial and unknown otherwise. 
Note that (RE)ML methods can only work with scale parameter 1 for the Poisson and binomial cases.    
</td></tr>
<tr valign="top"><td><code>select</code></td>
<td>
If this is <code>TRUE</code> then <code>gam</code> can add an extra penalty to each term so 
that it can be penalized to zero.  This means that the smoothing parameter estimation that is 
part of fitting can completely remove terms from the model. If the corresponding 
smoothing parameter is estimated as zero then the extra penalty has no effect.
</td></tr>
<tr valign="top"><td><code>knots</code></td>
<td>
this is an optional list containing user specified knot values to be used for basis construction. 
For most bases the user simply supplies the knots to be used, which must match up with the <code>k</code> value
supplied (note that the number of knots is not always just <code>k</code>). 
See <code><a href="smooth.construct.tp.smooth.spec.html">tprs</a></code> for what happens in the <code>"tp"/"ts"</code> case. 
Different terms can use different numbers of knots, unless they share a covariate.
</td></tr>
<tr valign="top"><td><code>sp</code></td>
<td>
A vector of smoothing parameters can be provided here.
Smoothing parameters must be supplied in the order that the smooth terms appear in the model 
formula. Negative elements indicate that the parameter should be estimated, and hence a mixture 
of fixed and estimated parameters is possible. If smooths share smoothing parameters then <code>length(sp)</code> 
must correspond to the number of underlying smoothing parameters.</td></tr>
<tr valign="top"><td><code>min.sp</code></td>
<td>
Lower bounds can be supplied for the smoothing parameters. Note
that if this option is used then the smoothing parameters <code>full.sp</code>, in the 
returned object, will need to be added to what is supplied here to get the 
smoothing parameters actually multiplying the penalties. <code>length(min.sp)</code> should 
always be the same as the total number of penalties (so it may be longer than <code>sp</code>,
if smooths share smoothing parameters).</td></tr>
<tr valign="top"><td><code>H</code></td>
<td>
A user supplied fixed quadratic penalty on the parameters of the 
GAM can be supplied, with this as its coefficient matrix. A common use of this term is 
to add a ridge penalty to the parameters of the GAM in circumstances in which the model
is close to un-identifiable on the scale of the linear predictor, but perfectly well
defined on the response scale.</td></tr>
<tr valign="top"><td><code>gamma</code></td>
<td>
It is sometimes useful to inflate the model degrees of 
freedom in the GCV or UBRE/AIC score by a constant multiplier. This allows 
such a multiplier to be supplied. </td></tr>
<tr valign="top"><td><code>fit</code></td>
<td>
If this argument is <code>TRUE</code> then <code>gam</code> sets up the model and fits it, but if it is
<code>FALSE</code> then the model is set up and an object <code>G</code> containing what
would be required to fit is returned is returned. See argument <code>G</code>.</td></tr>
<tr valign="top"><td><code>paraPen</code></td>
<td>
optional list specifying any penalties to be applied to parametric model terms. 
<code><a href="gam.models.html">gam.models</a></code> explains more.</td></tr>
<tr valign="top"><td><code>G</code></td>
<td>
Usually <code>NULL</code>, but may contain the object returned by a previous call to <code>gam</code> with 
<code>fit=FALSE</code>, in which case all other arguments are ignored except for
<code>gamma</code>, <code>in.out</code>, <code>control</code>, <code>method</code> and <code>fit</code>.</td></tr>
<tr valign="top"><td><code>in.out</code></td>
<td>
optional list for initializing outer iteration. If supplied
then this must contain two elements: <code>sp</code> should be an array of
initialization values for all smoothing parameters (there must be a value for
all smoothing parameters, whether fixed or to be estimated, but those for
fixed s.p.s are not used); <code>scale</code> is the typical scale of the GCV/UBRE function,
for passing to the outer optimizer.</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
further arguments for 
passing on e.g. to <code>gam.fit</code> (such as <code>mustart</code>). </td></tr>
</table>

<h3>Details</h3>

<p>
A generalized additive model (GAM) is a generalized linear model (GLM) in which the linear 
predictor is given by a user specified sum of smooth functions of the covariates plus a 
conventional parametric component of the linear predictor. A simple example is:
</p><p align="center"><i>log(E(y_i))=f_1(x_1i)+f_2(x_2i)</i></p><p>
where the (independent) response variables <i>y_i~Poi</i>, and
<i>f_1</i> and <i>f_2</i> are smooth functions of covariates <i>x_1</i> and 
<i>x_2</i>. The log is an example of a link function. 
</p>
<p>
If absolutely any smooth functions were allowed in model fitting then maximum likelihood 
estimation of such models would invariably result in complex overfitting estimates of 
<i>f_1</i>  and <i>f_2</i>. For this reason the models are usually fit by 
penalized likelihood 
maximization, in which the model (negative log) likelihood is modified by the addition of 
a penalty for each smooth function, penalizing its `wiggliness'. To control the tradeoff 
between penalizing wiggliness and penalizing badness of fit each penalty is multiplied by 
an associated smoothing parameter: how to estimate these parameters, and 
how to practically represent the smooth functions are the main statistical questions 
introduced by moving from GLMs to GAMs. 
</p>
<p>
The <code>mgcv</code> implementation of <code>gam</code> represents the smooth functions using 
penalized regression splines, and by default uses basis functions for these splines that 
are designed to be optimal, given the number basis functions used. The smooth terms can be 
functions of any number of covariates and the user has some control over how smoothness of 
the functions is measured. 
</p>
<p>
<code>gam</code> in <code>mgcv</code> solves the smoothing parameter estimation problem by using the 
Generalized Cross Validation (GCV) criterion
</p><p align="center"><i>n D/(n - DoF)^2</i></p><p>
or an Un-Biased Risk Estimator (UBRE )criterion
</p><p align="center"><i>D/n + 2 s DoF / n -s</i></p><p> 
where <i>D</i> is the deviance, <i>n</i> the number of data, <i>s</i>
the scale parameter and 
<i>DoF</i> the effective degrees of freedom of the model. Notice that UBRE is effectively
just AIC rescaled, but is only used when <i>s</i> is known. 
</p>
<p>
Alternatives are GACV, or a Laplace approximation to REML, there
is some evidence that the latter may actually be the most effective choise. 
</p>
<p>
Smoothing parameters are chosen to 
minimize the GCV, UBRE/AIC, GACV or REML scores for the model, and the main computational challenge solved 
by the <code>mgcv</code> package is to do this efficiently and reliably. Various
alternative numerical methods are provided which can be set by argument <code>optimizer</code>.
</p>
<p>
Broadly <code>gam</code> works by first constructing basis functions and one or more quadratic penalty 
coefficient matrices for each smooth term in the model formula, obtaining a model matrix for 
the strictly parametric part of the model formula, and combining these to obtain a 
complete model matrix (/design matrix) and a set of penalty matrices for the smooth terms. 
Some linear identifiability constraints are also obtained at this point. The model is 
fit using <code><a href="gam.fit.html">gam.fit</a></code>, a modification of <code><a href="../../stats/html/glm.html">glm.fit</a></code>. The GAM 
penalized likelihood maximization problem is solved by Penalized Iteratively 
Reweighted  Least Squares (P-IRLS) (see e.g. Wood 2000). 
Smoothing parameter selection is integrated in one of two ways. (i)
`Performance iteration' uses the fact that at each P-IRLS iteration a penalized 
weighted least squares problem is solved, and the smoothing parameters of that problem can 
estimated by GCV or UBRE. Eventually, in most cases, both model parameter estimates and smoothing 
parameter estimates converge. (ii) Alternatively the P-IRLS scheme is iterated to
convergence for each trial set of smoothing parameters, and GCV, UBRE or REML scores
are only evaluated on convergence - optimization is then `outer' to the P-IRLS
loop: in this case the P-IRLS iteration has to be differentiated, to
facilitate optimization, and <code><a href="gam.fit3.html">gam.fit3</a></code> is used in place of
<code>gam.fit</code>. The default is the second method, outer iteration.
</p>
<p>
Several alternative basis-penalty types  are built in for representing model
smooths, but alternatives can easily be added (see <code><a href="smooth.terms.html">smooth.terms</a></code> 
for an overview and <code><a href="smooth.construct.html">smooth.construct</a></code> for how to add smooth classes). In practice the 
default basis is usually the best choise, but the choise of the basis dimension (<code>k</code> in the 
<code>s</code> and <code>te</code> terms) is something that should be considered carefully (the exact value is not critical,
but it is important not to make it restrictively small, nor very large and computationally costly). The basis should 
be chosen to be larger than is believed to be necessary to approximate the smooth function concerned. 
The effective degrees of freedom for the smooth will then be controlled by the smoothing penalty on 
the term, and (usually) selected automatically (withy an upper limit set by
<code>k-1</code> or occasionally <code>k</code>). Of course 
the <code>k</code> should not be made too large, or computation will be slow (or in
extreme cases there will be more 
coefficients to estimate than there are data).
</p>
<p>
Note that <code>gam</code> assumes a very inclusive definition of what counts as a GAM: 
basically any penalized GLM can be used: to this end <code>gam</code> allows the non smooth model 
components to be penalized via argument <code>paraPen</code> and allows the linear predictor to depend on 
general linear functionals of smooths, via the summation convention mechanism described in 
<code><a href="linear.functional.terms.html">linear.functional.terms</a></code>.
</p>
<p>
Details of the default underlying fitting methods are given in Wood (2004
and 2008). Some alternative methods are discussed in Wood (2000 and 2006).
</p>


<h3>Value</h3>

<p>
If <code>fit=FALSE</code> the function returns a list <code>G</code> of items needed to
fit a GAM, but doesn't actually fit it. 
<br>
Otherwise the function returns an object of class <code>"gam"</code> as described in <code><a href="gamObject.html">gamObject</a></code>.</p>

<h3>WARNINGS</h3>

<p>
You must have more unique combinations of covariates than the model has total
parameters. (Total parameters is sum of basis dimensions plus sum of non-spline 
terms less the number of spline terms). 
</p>
<p>
Automatic smoothing parameter selection is not likely to work well when 
fitting models to very few response data.
</p>
<p>
For data with many  zeroes clustered together in the covariate space it is quite easy to set up 
GAMs which suffer from identifiability problems, particularly when using Poisson or binomial
families. The problem is that with e.g. log or logit links, mean value zero corresponds to
an infinite range on the linear predictor scale.
</p>


<h3>Author(s)</h3>

<p>
Simon N. Wood <a href="mailto:simon.wood@r-project.org">simon.wood@r-project.org</a>
</p>
<p>
Front end design inspired by the S function of the same name based on the work
of Hastie and Tibshirani (1990). Underlying methods owe much to the work of
Wahba (e.g. 1990) and Gu (e.g. 2002).
</p>


<h3>References</h3>

<p>
Key References on this implementation:
</p>
<p>
Wood, S.N. (2004) Stable and efficient multiple smoothing parameter estimation for
generalized additive models. J. Amer. Statist. Ass. 99:673-686. [Default
method for additive case (but no longer for generalized)]
</p>
<p>
Wood, S.N. (2008) Fast stable direct fitting and smoothness selection for generalized
additive models. J.R.Statist.Soc.B 70(3):495-518. [Generalized additive model case]
</p>
<p>
Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B 65(1):95-114
</p>
<p>
Wood, S.N. (2006a) Low rank scale invariant tensor product smooths for
generalized additive mixed models. Biometrics 62(4):1025-1036
</p>
<p>
Wood S.N. (2006b) Generalized Additive Models: An Introduction with R. Chapman
and Hall/CRC Press.
</p>
<p>
Wood, S.N. (2006c) On confidence intervals for generalized additive models based on penalized regression splines. Australian and New Zealand Journal of Statistics. 48(4): 445-464.
</p>
<p>
Key Reference on GAMs and related models:
</p>
<p>
Hastie (1993) in Chambers and Hastie (1993) Statistical Models in S. Chapman
and Hall.
</p>
<p>
Hastie and Tibshirani (1990) Generalized Additive Models. Chapman and Hall.
</p>
<p>
Wahba (1990) Spline Models of Observational Data. SIAM 
</p>
<p>
Wood, S.N. (2000)  Modelling and Smoothing Parameter Estimation
with Multiple Quadratic Penalties. J.R.Statist.Soc.B 62(2):413-428 [The original
mgcv paper, but no longer the default methods.]
</p>
<p>
Background References:
</p>
<p>
Green and Silverman (1994) Nonparametric Regression and Generalized  Linear Models. Chapman and Hall.
</p>
<p>
Gu and Wahba (1991) Minimizing GCV/GML scores with multiple smoothing parameters via
the Newton method. SIAM J. Sci. Statist. Comput. 12:383-398
</p>
<p>
Gu (2002) Smoothing Spline ANOVA Models, Springer.
</p>
<p>
O'Sullivan, Yandall and Raynor (1986) Automatic smoothing of regression
functions in generalized linear models.
J. Am. Statist.Ass. 81:96-103 
</p>
<p>
Wood (2001) mgcv:GAMs and Generalized Ridge Regression for R. R News 1(2):20-25
</p>
<p>
Wood and Augustin (2002) GAMs with integrated model selection using penalized regression splines and applications 
to environmental modelling. Ecological Modelling 157:157-177
</p>
<p>
<a href="http://www.maths.bath.ac.uk/~sw283/">http://www.maths.bath.ac.uk/~sw283/</a>
</p>


<h3>See Also</h3>

<p>
<code><a href="mgcv-package.html">mgcv-package</a></code>, <code><a href="gamObject.html">gamObject</a></code>, <code><a href="gam.models.html">gam.models</a></code>, <code><a href="smooth.terms.html">smooth.terms</a></code>,
<code><a href="linear.functional.terms.html">linear.functional.terms</a></code>, <code><a href="s.html">s</a></code>,
<code><a href="te.html">te</a></code> <code><a href="predict.gam.html">predict.gam</a></code>,
<code><a href="plot.gam.html">plot.gam</a></code>, <code><a href="summary.gam.html">summary.gam</a></code>, <code><a href="gam.side.html">gam.side</a></code>,
<code><a href="gam.selection.html">gam.selection</a></code>,<code><a href="mgcv.html">mgcv</a></code>, <code><a href="gam.control.html">gam.control</a></code>
<code><a href="gam.check.html">gam.check</a></code>, <code><a href="linear.functional.terms.html">linear.functional.terms</a></code> <code><a href="negbin.html">negbin</a></code>, <code><a href="magic.html">magic</a></code>,<code><a href="vis.gam.html">vis.gam</a></code>
</p>


<h3>Examples</h3>

<pre>
library(mgcv)
set.seed(0) ## simulate some data... 
dat &lt;- gamSim(1,n=400,dist="normal",scale=2)
b&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
summary(b)
plot(b,pages=1,residuals=TRUE)  ## show partial residuals
plot(b,pages=1,seWithMean=TRUE) ## `with intercept' CIs

## same fit in two parts .....
G&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),fit=FALSE,data=dat)
b&lt;-gam(G=G)
print(b)

## change the smoothness selection method to REML
b0&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
plot(b,pages=1)

## ... and do automatic terms selection as well
b1&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,
       method="REML",select=TRUE)
plot(b1,pages=1)

## set the smoothing parameter for the first term, estimate rest ...
bp&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),sp=c(0.01,-1,-1,-1),data=dat)
plot(bp,pages=1)
## alternatively...
bp &lt;- gam(y~s(x0,sp=.01)+s(x1)+s(x2)+s(x3),data=dat)

# set lower bounds on smoothing parameters ....
bp&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
        min.sp=c(0.001,0.01,0,10),data=dat) 
print(b);print(bp)

# same with REML
bp&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
        min.sp=c(0.1,0.1,0,10),data=dat,method="REML") 
print(b0);print(bp)

## now a GAM with 3df regression spline term &amp; 2 penalized terms
b0&lt;-gam(y~s(x0,k=4,fx=TRUE,bs="tp")+s(x1,k=12)+s(x2,k=15),data=dat)
plot(b0,pages=1)

## now fit a 2-d term to x0,x1
b1&lt;-gam(y~s(x0,x1)+s(x2)+s(x3),data=dat)
par(mfrow=c(2,2))
plot(b1)


par(mfrow=c(1,1))
## now simulate poisson data...
dat &lt;- gamSim(1,n=400,dist="poisson",scale=.25)

b2&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,data=dat)
plot(b2,pages=1)

## repeat fit using performance iteration

b3&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
        data=dat,optimizer="perf")
plot(b3,pages=1)

## repeat using GACV as in Wood 2008...

b4&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
        data=dat,method="GACV.Cp",scale=-1)
plot(b4,pages=1)

## repeat using REML as in Wood 2008...

b5&lt;-gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=poisson,
        data=dat,method="REML")
plot(b5,pages=1)

 

## a binary example 
dat &lt;- gamSim(1,n=400,dist="binary",scale=.33)

lr.fit &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=binomial,data=dat)
## plot model components with truth overlaid in red
op &lt;- par(mfrow=c(2,2))
fn &lt;- c("f0","f1","f2","f3");xn &lt;- c("x0","x1","x2","x3")
for (k in 1:4) {
  plot(lr.fit,residuals=TRUE,select=k)
  ff &lt;- dat[[fn[k]]];xx &lt;- dat[[xn[k]]]
  ind &lt;- sort.int(xx,index.return=TRUE)$ix
  lines(xx[ind],(ff-mean(ff))[ind]*.33,col=2)
}
par(op)
anova(lr.fit)
lr.fit1 &lt;- gam(y~s(x0)+s(x1)+s(x2),family=binomial,data=dat)
lr.fit2 &lt;- gam(y~s(x1)+s(x2),family=binomial,data=dat)
AIC(lr.fit,lr.fit1,lr.fit2)

## now a 2D smoothing example...

eg &lt;- gamSim(2,n=500,scale=.1)
attach(eg)

op &lt;- par(mfrow=c(2,2),mar=c(4,4,1,1))

contour(truth$x,truth$z,truth$f) ## contour truth
b4 &lt;- gam(y~s(x,z),data=data) ## fit model
fit1 &lt;- matrix(predict.gam(b4,pr,se=FALSE),40,40)
contour(truth$x,truth$z,fit1)   ## contour fit
persp(truth$x,truth$z,truth$f)    ## persp truth
vis.gam(b4)                     ## persp fit
detach(eg)
par(op)

## very large dataset example with user defined knots
par(mfrow=c(1,1))
eg &lt;- gamSim(2,n=10000,scale=.5)
attach(eg)

ind&lt;-sample(1:10000,1000,replace=FALSE)
b5&lt;-gam(y~s(x,z,k=50),data=data,
        knots=list(x=data$x[ind],z=data$z[ind]))
vis.gam(b5)

## and a pure "knot based" spline of the same data
b6&lt;-gam(y~s(x,z,k=100),data=data,knots=list(x= rep((1:10-0.5)/10,10),
        z=rep((1:10-0.5)/10,rep(10,10))))
vis.gam(b6,color="heat")

## varying the default large dataset behaviour via `xt'
b7 &lt;- gam(y~s(x,z,k=50,xt=list(max.knots=1000,seed=2)),data=data)
vis.gam(b7)
detach(eg)
</pre>



<hr><div align="center">[Package <em>mgcv</em> version 1.5-5 <a href="00Index.html">Index</a>]</div>

</body></html>
